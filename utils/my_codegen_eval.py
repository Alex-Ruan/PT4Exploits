#!/usr/bin/env python
import json
import sys
import os
import os.path
import re
import token
import tokenize
import argparse

import bleu_score
from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction
# Main function for CodaLab evaluation purposes
def main():

    p = argparse.ArgumentParser(description="Evaluator for CoNaLa",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter)

    p.add_argument("--input_dir",
                   help="input directory, containing 'res/answer.txt' and 'ref/truth.txt'",
                   default=None)
    p.add_argument("--input_ref",
                   help="input reference file",
                   default=None)
    p.add_argument("--input_hyp",
                   help="input hypothesis file",
                   default=None)
    p.add_argument("--output_file",
                   help="output score file",
                   default=None)
    p.add_argument("--output_dir",
                   help="output score directory which will contain output_dir/scores.txt",
                   default=None)
    p.add_argument("--no_exact_match",
                   help="only output bleu scores and not exact_match score",
                   action="store_true")
    p.add_argument("--strip_ref_metadata",
                   help="strip metadata from the reference and get only the code",
                   action="store_true")

    args = p.parse_args()

    # if not (args.input_dir or (args.input_ref and args.input_hyp)):
    #     raise ValueError("Must specify input_dir or input_ref+input_hyp")

    model_path='gen_model_pt_IP_blank'

    input_hyp = args.input_hyp if args.input_hyp else os.path.join(model_path, 'test_1.hyp')
    input_ref = args.input_ref if args.input_ref else os.path.join(model_path, 'test_1_true.gold')

    c_hyp, c_ref=[], []
    with open(input_hyp, 'r') as f_hyp:
        for line in f_hyp.readlines():
            c_hyp.append(tokenize_for_bleu_eval(line.strip()))

    with open(input_ref, 'r') as f_ref:
        for line in f_ref.readlines():
            c_ref.append(tokenize_for_bleu_eval(line.strip()))


    if len(c_hyp) != len(c_ref):
        raise ValueError('Length of hypothesis and reference don\'t match: {} != {}'.format(len(c_hyp), len(c_ref)))

    if args.output_file:
        f_out = open(args.output_file, 'w')
    elif args.output_dir:
        f_out = open(os.path.join(args.output_dir, 'scores.txt'), 'w')
    else:
        f_out = sys.stdout
    sm = SmoothingFunction()
    for i in range(1, 5):
        we=[1/i]*i
        ref_cpt=[[x] for x in c_ref]#本来里面的每一项都是list，现在变成list of list[[]]
        # print('corpus_bleu:',corpus_bleu(ref_cpt, c_hyp, smoothing_function=sm.method2))
        bleu=corpus_bleu(ref_cpt, c_hyp, weights=we, smoothing_function=None)
        print('corpus_bleu_{0}:{1:.2f}'.format(i, bleu*100))


    for i in range (1,5):#[['_start',':']]
        ref_cpt=[[x] for x in c_ref]#本来里面的每一项都是list，现在变成list of list[[]]
        bleu_tup = bleu_score.compute_bleu(ref_cpt, c_hyp, smooth=False, max_order = i)#语料库级别的bleu，把所有字符都分开了不包括_，但是包括,+-
        #(bleu, precisions, bp, ratio, translation_length, reference_length)#ratio:0.6024293389395001
        bleu = bleu_tup[0]
        f_out.write('bleu-' + str(i) + ':{0:.2f}\n'.format(bleu * 100)) 

    exact = sum([1 if h == r else 0 for h, r in zip(c_hyp, c_ref)])/len(c_hyp)

    if not args.no_exact_match:
        f_out.write('exact:{0:.2f}\n'.format(exact * 100))

    f_out.close()

""" Parses a file in the natural .jsonl format that the Conala corpus comes in.
    @param f: .jsonl file containing snippets
    @return: list of lists of tokens
"""
def parse_file_json(f):
    snippet_list = json.load(f)
    result = []
    for snippet in snippet_list:
        toks = tokenize_for_bleu_eval(snippet['snippet'])
        result.append(toks)
    return result

""" The tokenizer that we use for code submissions, from Wang Ling et al., Latent Predictor Networks for Code Generation (2016)
    @param code: string containing a code snippet
    @return: list of code tokens
"""
def tokenize_for_bleu_eval(code):
    code = re.sub(r'([^A-Za-z0-9_])', r' \1 ', code)#把\\n分开了
    code = re.sub(r'([a-z])([A-Z])', r'\1 \2', code)
    code = re.sub(r'\s+', ' ', code)#去掉空格
    code = code.replace('"', '`')
    code = code.replace('\'', '`')
    code = code.replace('\\n', '\n')
    # code=code.strip().lower()
    tokens = [t for t in code.split(' ') if t]

    return tokens

""" This runs the built-in Python tokenizer. Note that it only works on correctly parseable Python programs.
    @param string: string containing a Python tokenizable code snippet
    @return: list of code tokens
"""
def tokenize_code(string, concat_symbol=None):
    tokens = []
    string = string.strip().decode('utf-8').encode('ascii', 'strict') #.decode('string_escape')
    for toknum, tokval, _, _, _  in tokenize.generate_tokens(StringIO(string).readline):
        # We ignore these tokens during evaluation.
        if toknum not in [token.ENDMARKER, token.INDENT, token.DEDENT]:
            tokens.append(tokval.lower())

    return tokens

""" This builds the reference list for BLEU scoring
    @param reference_file_name: The reference file can be downloaded from https://conala-corpus.github.io/ as
                                conala_annotations.v1.0.zip/examples.annotated.test.json
    @return: list of references ready for BLEU scoring
"""
# 
def get_reference_list(reference_file_name):
    f_reference = open(reference_file_name)
    a = parse_file_json(f_reference)
    a = [[l] for l in a]
    return a

""" This scores hypotheses against references using BLEU.
    @param reference_list: reference list returned by get_reference_list.
    @param hypothesis_list: list of lists of tokens that a model generates.
    @return: 3-Tuple with the BLEU score, n-gram precisions, geometric mean of n-gram
             precisions and brevity penalty.
"""
def evaluate_bleu(reference_list, hypothesis_list):
    b = [tokenize_for_bleu_eval(s) for s in hypothesis_list]
    return bleu_score.compute_bleu(reference_list, b, smooth=False)

if __name__ == '__main__':
    main()
